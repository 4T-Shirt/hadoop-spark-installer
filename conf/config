
#配置原则：只提供最基本的配置，让系统可以跑起来，其它的过后再调整
#

# 格式要求：
# key = value


# 机器分为两类：
# master: 管理或辅助节点，要求高可用，建议使用RAID，其它硬件条件普通即可
# slave : 工作节点，担负集群主要的工作负载，比如 hadoop 存储节点，spark 计算节点

# TODO: 机器强制分为 2 类

ssh_port = 22

# default password of root for all hosts
root_passwd_def = 1234

# also support specify one by one
# root_passwd_192.168.0.1 = 1234
# root_passwd_m2.hadoop = 1234

# time server
ntp.server = 192.168.199.1

package.jdk = jdk-7u65-linux-x64.tar.gz
package.zookeeper = zookeeper-3.4.6.tar.gz
package.hadoop = hadoop-2.6.0.tar.gz
package.spark = spark-1.2.0-bin-hadoop2.4.tgz



#
#集群安装根目录
#
basedir.install = /usr/local/myhadoop
#
# 所有 log 放在一个目录下，方便管理
basedir.log = /usr/local/myhadoop/logs
#
# 所有 data 放在一个目录下，方便管理
basedir.data = /usr/local/myhadoop/datas

#
# 所有服务使用相同的账号运行
#
run.user = hdfs
run.group = hadoop

# master
# 管理集群的节点，每个
admin.hostnames = m1.hadoop

# master
# zookeeper，用于协调
# 机器数量：2n+1
# 必选
zookeeper.hostnames = m1.hadoop,m2.hadoop,m3.hadoop

#
# hadoop
# 必选
#
# master
# namenode，must 2
hadoop.namenode.hostnames = m1.hadoop,m2.hadoop
#
# slave
# datanode, > 1 个
hadoop.datanode.hostnames = m1.hadoop,m2.hadoop,m3.hadoop
#
# master
# JournalNode, 2*n+1, Strongly recommended to use the same as zookeeper
hadoop.journalnode.hostnames = m1.hadoop,m2.hadoop,m3.hadoop
#
# base data dir for every datanode
#   ext3 or ext4, 每个目录属于不同的物理磁盘
hadoop.datanode.databasedirs = /data1,/data2

#
# spark
# 必选
#
# master
# master, 2~3
spark.master.hostnames = m2.hadoop,m3.hadoop
#
# slave
# >1,  suggest same to hadoop.datanode.hostnames
spark.slave.hostnames = m1.hadoop,m2.hadoop,m3.hadoop

#
# client，那些需要访问集群服务的机器
# 
# 可选
#
# client.hostnames = client1.hadoop


